---
title: "testing"
author: "Tim"
date: "February 23, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = TRUE)
```

## Ensemble of Random Forests

Idea: Norm/stan transform data by column and fit an rf model. Then norm/stan transform by row and fit another model. Test

```{r}
library(tidyverse)
library(recipes)
library(caret)
library(rsample)

library(doMC)
registerDoMC(cores = 4)

set.seed(123)
seeds <- vector(mode = "list", length = 11)

for(i in 1:10) seeds[[i]]<- sample.int(n=1000, 4)
seeds[[11]] <- sample.int(1000, 1)

```
```{r}
train_data <- read_csv('../../train.csv')
test_data <- read_csv('../../test.csv')
```

Splits for testing:
```{r}
split <- initial_split(train_data, prop = 0.7)
train_data <- training(split)
test_data <- testing(split)
```


## Model 1: norm_std

```{r}
paint_recipe <- recipe(class ~ ., data = head(train_data))

paint_recipe <- paint_recipe %>%
  step_center(all_predictors()) %>%
  step_scale(all_predictors())

paint_trained <- prep(paint_recipe, training = train_data)

training_data <- bake(paint_trained, newdata = train_data)
testing_data <- bake(paint_trained, newdata = test_data)
test_1 <- testing_data
```

```{r}
control <- trainControl(
  method = "cv", 
  number = 10, 
  savePredictions = 'final',
  returnResamp = 'final',
  classProbs = TRUE,
  seeds = seeds)

rf_1 <- train(
  class ~ ., 
  data = training_data, 
  method = 'rf', 
  trControl = control)
```

```{r}
rf_1
```

```{r}
confusion_matrix <- function(model, data) {
  pred <- predict(model, newdata = data)
  actual <- data$class
  as.matrix(table(pred, actual))
}
```
```{r}
t <- confusion_matrix(rf_1, testing_data)
t
```

Accurary:

```{r}
sum(diag(t))/sum(t)
```

Per class performance:
```{r}
acc_class <- diag(t)/apply(t, 1, sum)
pre_class <- diag(t)/apply(t, 2, sum)
data.frame(correct = diag(t), total = apply(t, 2, sum),  acc = acc_class, pre = pre_class)
```

## Model 2: by row

```{r}
scale_data <- function(data) {
  temp <- apply(data, 2, function(x) {x - mean(x)})
  as.tibble(apply(temp, 2, function(x) {x/sd(x)}))
}
training_data <- scale_data(train_data[,-1])
training_data$class <- train_data$class

testing_data <- scale_data(test_data[,-1])
testing_data$class <- test_data$class
test_2 <- testing_data
```

```{r}
control <- trainControl(
  method = "cv", 
  number = 10, 
  savePredictions = 'final',
  returnResamp = 'final',
  classProbs = TRUE,
  seeds = seeds)

rf_2 <- train(
  class ~ ., 
  data = training_data, 
  method = 'rf', 
  trControl = control)
```

```{r}
rf_2
```

```{r}
t <- confusion_matrix(rf_2, testing_data)
t
```

Accurary:

```{r}
sum(diag(t))/sum(t)
```

Per class performance:
```{r}
acc_class <- diag(t)/apply(t, 1, sum)
pre_class <- diag(t)/apply(t, 2, sum)
data.frame(correct = diag(t), total = apply(t, 2, sum),  acc = acc_class, pre = pre_class)
```


## Model 3: By Corr

```{r}
paint_recipe <- recipe(class ~ ., data = head(train_data))

paint_recipe <- paint_recipe %>%
  step_center(all_predictors()) %>%
  step_scale(all_predictors()) %>%
  step_corr(all_predictors())

paint_trained <- prep(paint_recipe, training = train_data)

training_data <- bake(paint_trained, newdata = train_data)
testing_data <- bake(paint_trained, newdata = test_data)
test_3 <- testing_data
```

```{r}
control <- trainControl(
  method = "cv", 
  number = 10, 
  savePredictions = 'final',
  returnResamp = 'final',
  classProbs = TRUE,
  seeds = seeds)

rf_3 <- train(
  class ~ ., 
  data = training_data, 
  method = 'rf', 
  trControl = control)
```

```{r}
rf_3
```

```{r}
t <- confusion_matrix(rf_3, testing_data)
t
```

Accurary:

```{r}
sum(diag(t))/sum(t)
```

Per class performance:
```{r}
acc_class <- diag(t)/apply(t, 1, sum)
pre_class <- diag(t)/apply(t, 2, sum)
data.frame(correct = diag(t), total = apply(t, 2, sum),  acc = acc_class, pre = pre_class)
```

## Model Correlation

Surprisingly low
```{r}
model_list <- list(norm_std = rf_1,
                   row = rf_2,
                   corr = rf_3)
results <- resamples(model_list)
summary(results)
```

```{r}
modelCor(results)
splom(results)
```

## Ensemble 
### out of folds
```{r}
pred_grab <- function(models, train_data, test_data = NULL) {
  if (!is.null(test_data)) {
    agg_data <- purrr::map2_dfc(
      models,
      test_data,
      ~predict(.x, .y))
    return(agg_data)
  }
  agg_data <- purrr::map_dfc(
    models,
    ~.$pred$pred[order(.$pred$rowIndex)])
  agg_data$class <- train_data$class
  agg_data
}
```

```{r}
pred_train <- pred_grab(model_list, training_data)
pred_test <- pred_grab(model_list, test_data = list(one = test_1, 
                                                    two = test_2, 
                                                    three = test_3))
pred_test$class <- testing_data$class
```

```{r}
prob_grab <- function(models, train_data, test_data = NULL) {
  if (!is.null(test_data)) {
    agg_data <- purrr::map2_dfc(
      models,
      test_data,
      ~predict(.x, .y, type = 'prob'))
    return(agg_data)
  }
  cols <- c('cold', 'dusk', 'flowers', 'impressions', 'oval', 'scene', 'trees', 'water')
  agg_data <- purrr::map_dfc(
    models,
    ~.$pred[order(.$pred$rowIndex), cols])
  agg_data$class <- train_data$class
  agg_data
}
```

```{r}
prob_train <- prob_grab(model_list, training_data)
prob_test <- prob_grab(model_list, test_data = list(one = test_1, 
                                                    two = test_2, 
                                                    three = test_3))
prob_test$class <- testing_data$class
```

### Prediction-based Ensemble
```{r}
control <- trainControl(
  method = "cv",
  number = 10,
  savePredictions = TRUE,
  classProbs = TRUE)

pred_ensemble <- train(
  class ~ .,
  data = pred_train,
  method = 'rf',
  trControl = control
)
```


```{r}
t <- confusion_matrix(pred_ensemble, pred_test)
t
```

Accurary:

```{r}
sum(diag(t))/sum(t)
```

Per class performance:
```{r}
acc_class <- diag(t)/apply(t, 1, sum)
pre_class <- diag(t)/apply(t, 2, sum)
data.frame(correct = diag(t), total = apply(t, 2, sum),  acc = acc_class, pre = pre_class)
```

### Probability-based ensemble

```{r}
control <- trainControl(
  method = "cv",
  number = 10,
  savePredictions = TRUE,
  classProbs = TRUE)

prob_ensemble <- train(
  class ~ .,
  data = prob_train,
  method = 'rf',
  trControl = control
)
```
```{r}
t <- confusion_matrix(prob_ensemble, prob_test)
t
```

Accurary:

```{r}
sum(diag(t))/sum(t)
```

Per class performance:
```{r}
acc_class <- diag(t)/apply(t, 1, sum)
pre_class <- diag(t)/apply(t, 2, sum)
data.frame(correct = diag(t), total = apply(t, 2, sum),  acc = acc_class, pre = pre_class)
```

## Note to self:
tomorrow try with C50 model in this workspace and see if I can resample it

## C5.0 model

Test with resample
```{r}
training_data <- bake(paint_trained, newdata = train_data)
testing_data <- bake(paint_trained, newdata = test_data)
```

```{r}
tranformed_data <- function(train_data, test_data) {
  
}
```




```{r}
control <- trainControl(
  method = "cv", 
  number = 10, 
  savePredictions = 'final',
  returnResamp = 'final',
  classProbs = TRUE,
  seeds = seeds)

c50 <- train(
  class ~ ., 
  data = training_data, 
  method = 'rf', 
  trControl = control)
```

```{r}
c50
```

```{r}
results <- resamples(list(c50 = c50, rf = rf_1))
summary(results)
modelCor(results)
```





